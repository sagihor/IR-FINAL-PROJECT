{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "BUCKET_NAME = \"ir-sagi-bucket\"\n",
        "\n",
        "#!gsutil -m rm -r gs://{BUCKET_NAME}/postings_gcp || true\n",
        "#!gsutil -m rm -r gs://{BUCKET_NAME}/pr || true\n",
        "#!gsutil -m rm -r gs://{BUCKET_NAME}/notebooks || true\n",
        "#!gsutil -m rm -r gs://{BUCKET_NAME}/ir_project_indexes || true\n",
        "\n",
        "#!gsutil -m mkdir gs://{BUCKET_NAME}/ir_project_indexes || true\n"
      ],
      "metadata": {
        "id": "OI2qK47bfny0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud dataproc clusters list --region us-central1\n",
        "!pip install -q google-cloud-storage==1.43.0\n",
        "!pip install -q graphframes || true\n",
        "!ls -l /usr/lib/spark/jars/graph*\n"
      ],
      "metadata": {
        "id": "q1ndn3tdaI--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark\n",
        "import hashlib\n",
        "import sys\n",
        "from collections import Counter, OrderedDict, defaultdict\n",
        "import itertools\n",
        "from itertools import islice, count, groupby\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "from operator import itemgetter\n",
        "import nltk\n",
        "from nltk.stem.porter import *\n",
        "from nltk.corpus import stopwords\n",
        "from time import time\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "from google.cloud import storage\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "words = stopwords.words('english')\n",
        "\n",
        "with open(\"stopwords_en.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    for w in words:\n",
        "        f.write(w + \"\\n\")\n",
        "\n",
        "!gsutil cp stopwords_en.txt gs://ir-sagi-bucket/ir_project_indexes/stopwords_en.txt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RktZsuTrm_I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _hash(s: str) -> str:\n",
        "    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n",
        "\n",
        "RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n",
        "english_stopwords = frozenset(stopwords.words('english'))\n",
        "corpus_stopwords = frozenset([\n",
        "    \"category\", \"references\", \"also\", \"external\", \"links\",\n",
        "    \"may\", \"first\", \"see\", \"history\", \"people\", \"one\", \"two\",\n",
        "    \"part\", \"thumb\", \"including\", \"second\", \"following\",\n",
        "    \"many\", \"however\", \"would\", \"became\"\n",
        "])\n",
        "all_stopwords = english_stopwords.union(corpus_stopwords)\n",
        "\n",
        "def tokenize(text: str):\n",
        "    tokens = [m.group() for m in RE_WORD.finditer(text.lower())]\n",
        "    return [t for t in tokens if t not in all_stopwords]\n"
      ],
      "metadata": {
        "id": "1AxViRYu30-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf, SparkFiles\n",
        "from pyspark.sql import SQLContext\n",
        "from graphframes import *"
      ],
      "metadata": {
        "id": "DJ5OSr9yaRL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "id": "d_Bx_r3zaRD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bucket_name = 'ir-sagi-bucket'\n",
        "full_path = f\"gs://{bucket_name}/\"\n",
        "paths = []\n",
        "\n",
        "client = storage.Client()\n",
        "blobs = client.list_blobs(bucket_name)\n",
        "\n",
        "for b in blobs:\n",
        "    if b.name == 'graphframes.sh':\n",
        "        continue\n",
        "    if b.name.startswith('ir_project_indexes/'):\n",
        "        continue\n",
        "\n",
        "    paths.append(full_path + b.name)\n",
        "\n",
        "print(\"num files =\", len(paths))\n",
        "\n"
      ],
      "metadata": {
        "id": "kyLogeSUadxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "parquetFile = spark.read.parquet(*paths)\n",
        "doc_text_pairs = parquetFile.select(\"text\", \"id\").rdd"
      ],
      "metadata": {
        "id": "1G2dNqQXbifJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count number of wiki pages\n",
        "N_DOCS = parquetFile.count()\n",
        "print(N_DOCS)"
      ],
      "metadata": {
        "id": "uMTKTksfqLv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if nothing prints here you forgot to upload the file inverted_index_gcp.py to the home dir\n",
        "%cd -q /home/dataproc\n",
        "!ls inverted_index_gcp.py"
      ],
      "metadata": {
        "id": "Xna7cGkgqN3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding our python module to the cluster\n",
        "sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n",
        "sys.path.insert(0,SparkFiles.getRootDirectory())"
      ],
      "metadata": {
        "id": "M1eNe0xZqTh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inverted_index_gcp import InvertedIndex"
      ],
      "metadata": {
        "id": "kZ9xV6QFqXRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**END OF SETUP**"
      ],
      "metadata": {
        "id": "UmUbMVb7rulz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**avgdl_body**"
      ],
      "metadata": {
        "id": "VA0qJ4kEbwy7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = BUCKET_NAME\n",
        "BASE_GCS_DIR = \"ir_project_indexes\"\n",
        "BODY_BASE_DIR = \"ir_project_indexes/body_index\"\n",
        "\n",
        "# =========================\n",
        "# Build AVGDL (single file) on GCS\n",
        "# =========================\n",
        "\n",
        "import tempfile, os, glob, subprocess\n",
        "\n",
        "# doc_text_pairs: RDD of (text, doc_id)\n",
        "\n",
        "dl_rdd = doc_text_pairs.map(lambda x: len(tokenize(x[0])))\n",
        "\n",
        "total_dl = dl_rdd.map(int).sum()\n",
        "doc_cnt  = dl_rdd.count()\n",
        "avgdl = total_dl / float(doc_cnt) if doc_cnt else 0.0\n",
        "\n",
        "print(\"DOC_COUNT =\", doc_cnt)\n",
        "print(\"TOTAL_DL  =\", total_dl)\n",
        "print(\"AVGDL     =\", avgdl)\n",
        "\n",
        "tmp_dir = tempfile.mkdtemp(prefix=\"avgdl_\")\n",
        "local_avgdl = os.path.join(tmp_dir, \"avgdl_body.txt\")\n",
        "\n",
        "with open(local_avgdl, \"w\") as f:\n",
        "    f.write(f\"{avgdl}\\n\")\n",
        "\n",
        "AVGDL_GCS_PATH = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/avgdl_body.txt\"\n",
        "subprocess.check_call([\"gsutil\", \"cp\", local_avgdl, AVGDL_GCS_PATH])\n",
        "\n",
        "print(\"Uploaded AVGDL to:\", AVGDL_GCS_PATH)\n"
      ],
      "metadata": {
        "id": "AXMjSbNjbzbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**body_index**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SgbfCh5_8MU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bucket_name = BUCKET_NAME\n",
        "BASE_GCS_DIR = \"ir_project_indexes\"\n",
        "BODY_BASE_DIR = \"ir_project_indexes/body_index\"\n"
      ],
      "metadata": {
        "id": "QYXu6n61rzxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate and write the dl_body (dict of doc_id -> length), and avgdl (average document length)\n",
        "#client = storage.Client()\n",
        "#bucket = client.bucket(BUCKET_NAME)\n",
        "\n",
        "# create tuples of (doc_id, length)\n",
        "#doc_len_rdd = doc_text_pairs.map(lambda x: (x[1], len(tokenize(x[0]))))\n",
        "# dictionary doc_id -> length\n",
        "#dl_body = dict(doc_len_rdd.collect())\n",
        "#avgdl_body = sum(dl_body.values()) /  (len(dl_body))\n",
        "\n",
        "#with bucket.blob(f\"{BASE_GCS_DIR}/dl_body.pkl\").open(\"wb\") as f:\n",
        " #   pickle.dump(dl_body, f)\n",
        "\n",
        "#with bucket.blob(f\"{BASE_GCS_DIR}/avgdl_body.pkl\").open(\"wb\") as f:\n",
        " #   pickle.dump(avgdl_body, f)"
      ],
      "metadata": {
        "id": "Gica5CvtjBVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_BUCKETS = 124\n",
        "def token2bucket_id(token):\n",
        "    return int(_hash(token), 16) % NUM_BUCKETS"
      ],
      "metadata": {
        "id": "Y5Si46gSmOki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from operator import add\n",
        "\n",
        "def word_count(text, doc_id):\n",
        "    tokens = tokenize(text)\n",
        "    if not tokens:\n",
        "        return []\n",
        "    tf_dict = Counter(tokens)\n",
        "    return [(token, (doc_id, tf)) for token, tf in tf_dict.items()]\n",
        "\n",
        "def reduce_word_counts(unsorted_pl):\n",
        "    # sort the list by wiki_id\n",
        "    return sorted(unsorted_pl, key=lambda x: x[0])\n",
        "\n",
        "def calculate_df(postings):\n",
        "    # postings is a rdd of (token, posting_list) pair. to know what is the token's df we take the length of its posting list.\n",
        "    return postings.map(lambda x: (x[0], len(x[1])))\n",
        "\n",
        "def partition_postings_and_write(postings, relevant_base_dir):\n",
        "    # we get an RDD where each item is a (w, posting_list) pair.\n",
        "    # first: calculate the appropriate bucket for each w and group by the bucked id.\n",
        "    # then: each rdd element is (bucket_id, iterator of (w, posting_list) that is allocated to the bucket).\n",
        "    # now: make the iterator to a list, and write each bucket's posting lists to disk by bucket id using the function.\n",
        "    return postings.map(lambda x: (token2bucket_id(x[0]), x)).groupByKey().map(lambda bucket_and_its_posting_lists: InvertedIndex.write_a_posting_list(bucket_and_its_posting_lists, relevant_base_dir, bucket_name))"
      ],
      "metadata": {
        "id": "t0vHnOEQmtYE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word counts map\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)\n",
        "w2df = calculate_df(postings_filtered)\n",
        "w2df_dict = w2df.collectAsMap()\n",
        "# partition posting lists and write outs\n",
        "_ = partition_postings_and_write(postings_filtered, BODY_BASE_DIR).collect()\n"
      ],
      "metadata": {
        "id": "wSGXbv2B2R43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect all posting lists locations into one super-set\n",
        "super_posting_locs = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix=BODY_BASE_DIR):\n",
        "  if not blob.name.endswith(\"pickle\"):\n",
        "    continue\n",
        "  with blob.open(\"rb\") as f:\n",
        "    posting_locs = pickle.load(f)\n",
        "    for k, v in posting_locs.items():\n",
        "      super_posting_locs[k].extend(v)"
      ],
      "metadata": {
        "id": "-Vwpvt052bcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create inverted index instance\n",
        "inverted = InvertedIndex()\n",
        "# Adding the posting locations dictionary to the inverted index\n",
        "inverted.posting_locs = super_posting_locs\n",
        "# Add the token - df dictionary to the inverted index\n",
        "inverted.df = w2df_dict\n",
        "# write the global stats out\n",
        "inverted.write_index('.', 'body')\n",
        "# upload to gs\n",
        "index_src = \"body.pkl\"\n",
        "index_dst = f'gs://{bucket_name}/{BODY_BASE_DIR}/{index_src}'\n",
        "!gsutil cp $index_src $index_dst"
      ],
      "metadata": {
        "id": "VCxJhWQr3AtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TITLE_BASE_DIR = \"ir_project_indexes/title_index\"\n",
        "doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n",
        "#check:\n",
        "doc_title_pairs.take(3)"
      ],
      "metadata": {
        "id": "1Fzy5oJR-88L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**doc-id -> doc length mapping by tsv + idx**:"
      ],
      "metadata": {
        "id": "ckDsTe0xiDIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Build dl_body.tsv parts (sorted by id) on GCS\n",
        "# =========================\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "DL_TMP_DIR  = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/dl_body_tsv_parts\"\n",
        "DL_TSV_GCS  = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/dl_body.tsv\"\n",
        "DL_IDX_GCS  = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/dl_body.idx\"\n",
        "\n",
        "# doc_text_pairs: RDD of (text, id)  (כמו שיש לך כבר במחברת)\n",
        "dl_rdd = doc_text_pairs.map(lambda x: (int(x[1]), int(len(tokenize(x[0])))))\n",
        "dl_df = spark.createDataFrame(dl_rdd, [\"id\", \"dl\"])\n",
        "\n",
        "dl_sorted = dl_df.sort(\"id\")\n",
        "dl_sorted.selectExpr(\"concat(cast(id as string), '\\t', cast(dl as string)) as line\").write.mode(\"overwrite\").text(DL_TMP_DIR)\n",
        "\n",
        "print(\"Wrote DL TSV parts to:\", DL_TMP_DIR)\n"
      ],
      "metadata": {
        "id": "rmqpQSasiODO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, struct, subprocess, tempfile\n",
        "\n",
        "LOCAL_DIR = tempfile.mkdtemp(prefix=\"dl_body_build_\")\n",
        "LOCAL_TSV = os.path.join(LOCAL_DIR, \"dl_body.tsv\")\n",
        "LOCAL_IDX = os.path.join(LOCAL_DIR, \"dl_body.idx\")\n",
        "\n",
        "subprocess.check_call([\"gsutil\", \"-m\", \"cp\", f\"{DL_TMP_DIR}/part-*\", LOCAL_DIR])\n",
        "parts = sorted(glob.glob(os.path.join(LOCAL_DIR, \"part-*\")))\n",
        "\n",
        "with open(LOCAL_TSV, \"wb\") as out:\n",
        "    for p in parts:\n",
        "        with open(p, \"rb\") as f:\n",
        "            for line in f:\n",
        "                out.write(line.rstrip(b\"\\n\") + b\"\\n\")\n",
        "\n",
        "REC = struct.Struct(\"<I Q I\")  # uint32 doc_id | uint64 offset | uint32 length\n",
        "\n",
        "with open(LOCAL_TSV, \"rb\") as f_tsv, open(LOCAL_IDX, \"wb\") as f_idx:\n",
        "    offset = 0\n",
        "    for line in f_tsv:\n",
        "        length = len(line)\n",
        "        tab = line.find(b\"\\t\")\n",
        "        if tab < 0:\n",
        "            offset += length\n",
        "            continue\n",
        "        doc_id = int(line[:tab])\n",
        "        f_idx.write(REC.pack(doc_id, offset, length))\n",
        "        offset += length\n",
        "\n",
        "print(\"Local DL TSV/IDX:\", LOCAL_TSV, LOCAL_IDX)\n",
        "\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_TSV, DL_TSV_GCS])\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_IDX, DL_IDX_GCS])\n",
        "\n",
        "print(\"Uploaded:\", DL_TSV_GCS, DL_IDX_GCS)\n"
      ],
      "metadata": {
        "id": "Nlx5yjnNiN2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**doc_id -> doc norm : tsv + idx**:"
      ],
      "metadata": {
        "id": "UmRpqNjciYSN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RE GENERATE FOR THAT: (WITHOUT RUNNING BODY INDEX):\n",
        "word_counts = doc_text_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "# filtering postings and calculate df\n",
        "postings_filtered = postings.filter(lambda x: len(x[1])>50)"
      ],
      "metadata": {
        "id": "baW2d5H_ir5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from operator import add\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "N_DOCS = parquetFile.count()  # מספר מסמכים כולל (כמו DOC_COUNT בשרת)\n",
        "\n",
        "NORMS_TMP_DIR = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/docnorms_body_tsv_parts\"\n",
        "NORMS_TSV_GCS = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/docnorms_body.tsv\"\n",
        "NORMS_IDX_GCS = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/docnorms_body.idx\"\n",
        "\n",
        "def term_to_doc_squares(term_and_pl):\n",
        "    term, pl = term_and_pl\n",
        "    df = len(pl)\n",
        "    if df <= 0:\n",
        "        return []\n",
        "    # תואם לשרת אם הוא משתמש log(N/df) (או תחליף ל-smoothing אם תרצה)\n",
        "    idf = math.log(N_DOCS / df)\n",
        "    out = []\n",
        "    for doc_id, tf in pl:\n",
        "        if tf <= 0:\n",
        "            continue\n",
        "        w = (1.0 + math.log(tf)) * idf\n",
        "        out.append((int(doc_id), float(w*w)))\n",
        "    return out\n",
        "\n",
        "doc_sumsq = postings_filtered.flatMap(term_to_doc_squares).reduceByKey(add)\n",
        "doc_norms = doc_sumsq.mapValues(lambda s: math.sqrt(s))\n",
        "\n",
        "norms_df = spark.createDataFrame(doc_norms, [\"id\", \"norm\"]).sort(\"id\")\n",
        "norms_df.selectExpr(\"concat(cast(id as string), '\\t', cast(norm as string)) as line\") \\\n",
        "        .write.mode(\"overwrite\").text(NORMS_TMP_DIR)\n",
        "\n",
        "print(\"Wrote NORMS TSV parts to:\", NORMS_TMP_DIR)\n"
      ],
      "metadata": {
        "id": "dVZLH2b2i6ip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, struct, subprocess, tempfile\n",
        "\n",
        "LOCAL_DIR = tempfile.mkdtemp(prefix=\"docnorms_body_build_\")\n",
        "LOCAL_TSV = os.path.join(LOCAL_DIR, \"docnorms_body.tsv\")\n",
        "LOCAL_IDX = os.path.join(LOCAL_DIR, \"docnorms_body.idx\")\n",
        "\n",
        "subprocess.check_call([\"gsutil\", \"-m\", \"cp\", f\"{NORMS_TMP_DIR}/part-*\", LOCAL_DIR])\n",
        "parts = sorted(glob.glob(os.path.join(LOCAL_DIR, \"part-*\")))\n",
        "\n",
        "with open(LOCAL_TSV, \"wb\") as out:\n",
        "    for p in parts:\n",
        "        with open(p, \"rb\") as f:\n",
        "            for line in f:\n",
        "                out.write(line.rstrip(b\"\\n\") + b\"\\n\")\n",
        "\n",
        "REC = struct.Struct(\"<I Q I\")\n",
        "\n",
        "with open(LOCAL_TSV, \"rb\") as f_tsv, open(LOCAL_IDX, \"wb\") as f_idx:\n",
        "    offset = 0\n",
        "    for line in f_tsv:\n",
        "        length = len(line)\n",
        "        tab = line.find(b\"\\t\")\n",
        "        if tab < 0:\n",
        "            offset += length\n",
        "            continue\n",
        "        doc_id = int(line[:tab])\n",
        "        f_idx.write(REC.pack(doc_id, offset, length))\n",
        "        offset += length\n",
        "\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_TSV, NORMS_TSV_GCS])\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_IDX, NORMS_IDX_GCS])\n",
        "\n",
        "print(\"Uploaded:\", NORMS_TSV_GCS, NORMS_IDX_GCS)\n"
      ],
      "metadata": {
        "id": "VGIRsK_CjeVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate doc_id - > title mapping without exceeding**\n"
      ],
      "metadata": {
        "id": "XW-zECbeAj6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#titles.pkl\n",
        "#titles = dict(doc_title_pairs.map(lambda x: (x[1], x[0])).collect())\n",
        "\n",
        "#with bucket.blob(f\"{BASE_GCS_DIR}/titles.pkl\").open(\"wb\") as f:\n",
        " #   pickle.dump(titles, f)\n"
      ],
      "metadata": {
        "id": "aepDMniG_DXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Key Idea: The TSV file contains all document IDs and titles written sequentially.\n",
        "# The index maps each document ID to its exact byte offset in the TSV file.\n",
        "# After a binary search in the index, the title is read directly in O(1).\n",
        "\n",
        "# Build titles.tsv (sorted by id) as text files on GCS\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Temporary directory in GCS where Spark will write many part-files\n",
        "TITLES_TMP_DIR = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/titles_tsv_parts\"\n",
        "# Final output locations in GCS\n",
        "TITLES_TSV_GCS = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/titles.tsv\"\n",
        "TITLES_IDX_GCS = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/titles.idx\"\n",
        "\n",
        "# Select only document id and title from the parquet file\n",
        "titles_df = parquetFile.select(F.col(\"id\").cast(\"long\").alias(\"id\"), F.col(\"title\").cast(\"string\").alias(\"title\"))\n",
        "\n",
        "# Sort by document id so the index can be binary-searched later\n",
        "titles_sorted = titles_df.sort(\"id\")\n",
        "\n",
        "# Create one TSV line per document: \"id<TAB>title\"\n",
        "titles_sorted.selectExpr(\"concat(cast(id as string), '\\t', title) as line\").write.mode(\"overwrite\").text(TITLES_TMP_DIR)\n",
        "\n",
        "print(\"Wrote TSV parts to:\", TITLES_TMP_DIR)\n"
      ],
      "metadata": {
        "id": "KyWRJ94D6p_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, struct, subprocess, tempfile\n",
        "\n",
        "# Create a temporary local directory on the Spark driver\n",
        "LOCAL_DIR = tempfile.mkdtemp(prefix=\"titles_build_\")\n",
        "# Local paths for the merged TSV and the binary index\n",
        "LOCAL_TSV = os.path.join(LOCAL_DIR, \"titles.tsv\")\n",
        "LOCAL_IDX = os.path.join(LOCAL_DIR, \"titles.idx\")\n",
        "\n",
        "# Download all Spark part-files from GCS to the local driver disk\n",
        "subprocess.check_call([\"gsutil\", \"-m\", \"cp\", f\"{TITLES_TMP_DIR}/part-*\", LOCAL_DIR])\n",
        "\n",
        "# Collect and sort all part files by filename\n",
        "parts = sorted(glob.glob(os.path.join(LOCAL_DIR, \"part-*\")))\n",
        "\n",
        "# Merge all part-files into one TSV file\n",
        "with open(LOCAL_TSV, \"wb\") as out:\n",
        "    for p in parts:\n",
        "        with open(p, \"rb\") as f:\n",
        "            for line in f:\n",
        "                out.write(line.rstrip(b\"\\n\") + b\"\\n\")\n",
        "\n",
        "# Define binary index record format:\n",
        "# uint32 doc_id | uint64 offset | uint32 length  (16 bytes total)\n",
        "REC = struct.Struct(\"<I Q I\")\n",
        "\n",
        "# Build the index by scanning the TSV sequentially\n",
        "with open(LOCAL_TSV, \"rb\") as f_tsv, open(LOCAL_IDX, \"wb\") as f_idx:\n",
        "  # current byte offset in the TSV file\n",
        "    offset = 0\n",
        "    for line in f_tsv:\n",
        "        length = len(line)\n",
        "        tab = line.find(b\"\\t\")\n",
        "        if tab < 0:\n",
        "            offset += length\n",
        "            continue\n",
        "        doc_id = int(line[:tab])\n",
        "        f_idx.write(REC.pack(doc_id, offset, length))\n",
        "        # Move offset to the start of the next line\n",
        "        offset += length\n",
        "\n",
        "print(\"Local TSV/IDX:\", LOCAL_TSV, LOCAL_IDX)\n",
        "\n",
        "# Upload the merged TSV file to GCS\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_TSV, TITLES_TSV_GCS])\n",
        "# Upload the binary index file to GCS\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_IDX, TITLES_IDX_GCS])\n",
        "\n",
        "print(\"Uploaded:\", TITLES_TSV_GCS, TITLES_IDX_GCS)\n"
      ],
      "metadata": {
        "id": "dnRox2UQ6-j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check\n",
        "prev = -1\n",
        "bad_at = None\n",
        "with open(LOCAL_IDX, \"rb\") as f:\n",
        "    while True:\n",
        "        b = f.read(REC.size)\n",
        "        if not b:\n",
        "            break\n",
        "        doc_id, off, ln = REC.unpack(b)\n",
        "        if doc_id < prev:\n",
        "            bad_at = (prev, doc_id)\n",
        "            break\n",
        "        prev = doc_id\n",
        "\n",
        "print(\"OK sorted\" if bad_at is None else f\"NOT sorted: {bad_at[0]} -> {bad_at[1]}\")\n"
      ],
      "metadata": {
        "id": "lJAosVRzCkcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**title index**"
      ],
      "metadata": {
        "id": "uW6tA5WXvU7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# titles index\n",
        "\n",
        "# word counts map\n",
        "word_counts = doc_title_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "\n",
        "postings_filtered = postings.filter(lambda x: len(x[1]) > 50)\n",
        "w2df_dict = calculate_df(postings_filtered).collectAsMap()\n",
        "\n",
        "# write postings\n",
        "_ = partition_postings_and_write(postings_filtered, TITLE_BASE_DIR).collect()\n",
        "\n",
        "# merge posting_locs\n",
        "from collections import defaultdict\n",
        "super_posting_locs = defaultdict(list)\n",
        "for blob in client.list_blobs(bucket_name, prefix=TITLE_BASE_DIR):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs[k].extend(v)\n",
        "\n",
        "inverted = InvertedIndex()\n",
        "inverted.posting_locs = super_posting_locs\n",
        "inverted.df = w2df_dict\n",
        "\n",
        "inverted.write_index('.', 'title')\n",
        "!gsutil cp title.pkl gs://{bucket_name}/{TITLE_BASE_DIR}/title.pkl\n",
        "\n"
      ],
      "metadata": {
        "id": "XUPo_PuK_bRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANCHOR INDEX**"
      ],
      "metadata": {
        "id": "UizJn5c2mI7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ANCHOR_BASE_DIR = \"ir_project_indexes/anchor_index\"\n"
      ],
      "metadata": {
        "id": "BZOfHW40_3SW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# anchor_index = term -> [(doc_id, tf)]. doc_id is the id of each doc that has been reached with a link that the link's text contains the term.\n",
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import Row\n",
        "\n",
        "# pages:(source_id, anchor_array), anchor_array = array of structs, each struct:\n",
        "# id: long, text: string. id in the struct is target id.\n",
        "pages = parquetFile.select(\"id\", \"anchor_text\").rdd.map(lambda r: (r[\"id\"], r[\"anchor_text\"]))\n",
        "\n",
        "# create RDD of (text, target_id) for each anchor_text in anchor_array of specific source doc.\n",
        "# dont care about source_id\n",
        "doc_anchor_pairs = (pages.flatMap(lambda row: [] if row[1] is None else [(a[\"text\"], a[\"id\"]) for a in row[1] if a and a[\"text\"] and a[\"id\"] is not None]))\n",
        "\n"
      ],
      "metadata": {
        "id": "Lk-VJ5VenXhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word counts map\n",
        "word_counts = doc_anchor_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n",
        "\n",
        "# postings: token -> [(doc_id, tf), ...]\n",
        "postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n",
        "\n",
        "# filtering postings and calculate df (כמו A3)\n",
        "postings_filtered = postings.filter(lambda x: len(x[1]) > 50)\n",
        "\n",
        "w2df_dict = calculate_df(postings_filtered).collectAsMap()\n",
        "\n",
        "# write postings bins + posting_locs pickles\n",
        "_ = partition_postings_and_write(postings_filtered, ANCHOR_BASE_DIR).collect()\n"
      ],
      "metadata": {
        "id": "joOxhUNSsdQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = storage.Client()\n",
        "super_posting_locs = defaultdict(list)\n",
        "\n",
        "for blob in client.list_blobs(bucket_name, prefix=ANCHOR_BASE_DIR):\n",
        "    if not blob.name.endswith(\"pickle\"):\n",
        "        continue\n",
        "    with blob.open(\"rb\") as f:\n",
        "        posting_locs = pickle.load(f)\n",
        "        for k, v in posting_locs.items():\n",
        "            super_posting_locs[k].extend(v)\n",
        "\n",
        "# Create inverted index instance\n",
        "inverted = InvertedIndex()\n",
        "inverted.posting_locs = super_posting_locs\n",
        "inverted.df = w2df_dict\n",
        "\n",
        "# write the global stats out\n",
        "inverted.write_index('.', 'anchor')\n",
        "\n",
        "# upload to gs\n",
        "!gsutil cp anchor.pkl gs://{bucket_name}/{ANCHOR_BASE_DIR}/anchor.pkl\n"
      ],
      "metadata": {
        "id": "O9YINk7Qsi_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PageRank**"
      ],
      "metadata": {
        "id": "wDi2baAGoNVA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_graph(pages):\n",
        "\n",
        "    # the vertices are all the articles who point to other + those who are only pointed by others.\n",
        "    # we select to represent by a string tuple that contains the article_id only. that because of the dataframe creation later.\n",
        "    # take the articles i'ds for those who point to other\n",
        "    vertices = pages.map(lambda row: row[0])\n",
        "    # each row originally has a arcticle_id and a list of (target_id, text). need to create (article_id, target_id) tuples for all.\n",
        "    # for each (target_id, text) in the list we take the target id and ceate (article_id, target_id) tuple.\n",
        "    # distinct takes only different.\n",
        "    edges = pages.flatMap(lambda row: set([(row[0], target_id) for target_id, _ in row[1]]))\n",
        "    # add the articles id's for those who are only pointed by others\n",
        "    vertices = vertices.union(edges.map(lambda edge_expression: edge_expression[1]))\n",
        "    # distinct takes only different.\n",
        "    vertices = vertices.distinct()\n",
        "    vertices = vertices.map(lambda x: (x,))\n",
        "    return edges, vertices\n",
        "\n",
        "pages_links = spark.read.parquet(\"gs://ir-sagi-bucket/multistream*\").select(\"id\", \"anchor_text\").rdd\n",
        "# construct the graph\n",
        "edges, vertices = generate_graph(pages_links)\n",
        "# compute PageRank\n",
        "edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n",
        "verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n",
        "g = GraphFrame(verticesDF, edgesDF)\n",
        "pr = g.pageRank(resetProbability=0.15, maxIter=6)\n"
      ],
      "metadata": {
        "id": "fuOOtiVI3lEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PageRank.tsv and PageRank.idx**"
      ],
      "metadata": {
        "id": "Oyhg4y6E50S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "BASE_GCS_DIR = \"ir_project_indexes\"\n",
        "PR_TMP_DIR   = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/pagerank_tsv_parts\"\n",
        "PR_TSV_GCS   = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/pagerank.tsv\"\n",
        "PR_IDX_GCS   = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/pagerank.idx\"\n",
        "\n",
        "# (id, pagerank) + sort by id\n",
        "pr_df = pr.vertices.select(F.col(\"id\").cast(\"long\").alias(\"id\"),F.col(\"pagerank\").cast(\"double\").alias(\"pagerank\")).sort(\"id\")\n",
        "\n",
        "# Write \"id<TAB>pagerank\" as text parts\n",
        "(pr_df.selectExpr(\"concat(cast(id as string), '\\t', cast(pagerank as string)) as line\").write.mode(\"overwrite\").text(PR_TMP_DIR))\n",
        "\n",
        "print(\"Wrote PageRank TSV parts to:\", PR_TMP_DIR)\n",
        "print(\"Will upload final TSV/IDX to:\", PR_TSV_GCS, PR_IDX_GCS)\n"
      ],
      "metadata": {
        "id": "udA2ym3K5pun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, struct, subprocess, tempfile\n",
        "\n",
        "LOCAL_DIR = tempfile.mkdtemp(prefix=\"pagerank_build_\")\n",
        "LOCAL_TSV = os.path.join(LOCAL_DIR, \"pagerank.tsv\")\n",
        "LOCAL_IDX = os.path.join(LOCAL_DIR, \"pagerank.idx\")\n",
        "\n",
        "# Download all part files\n",
        "subprocess.check_call([\"gsutil\", \"-m\", \"cp\", f\"{PR_TMP_DIR}/part-*\", LOCAL_DIR])\n",
        "parts = sorted(glob.glob(os.path.join(LOCAL_DIR, \"part-*\")))\n",
        "\n",
        "# Merge parts into one TSV (preserves order if Spark global sort is used)\n",
        "with open(LOCAL_TSV, \"wb\") as out:\n",
        "    for p in parts:\n",
        "        with open(p, \"rb\") as f:\n",
        "            for line in f:\n",
        "                out.write(line.rstrip(b\"\\n\") + b\"\\n\")\n",
        "\n",
        "# Index record: uint32 doc_id | uint64 offset | uint32 length  (16 bytes)\n",
        "REC = struct.Struct(\"<I Q I\")\n",
        "\n",
        "# Build idx by scanning TSV sequentially\n",
        "with open(LOCAL_TSV, \"rb\") as f_tsv, open(LOCAL_IDX, \"wb\") as f_idx:\n",
        "    offset = 0\n",
        "    for line in f_tsv:\n",
        "        length = len(line)\n",
        "        tab = line.find(b\"\\t\")\n",
        "        if tab < 0:\n",
        "            offset += length\n",
        "            continue\n",
        "        doc_id = int(line[:tab])  # assumes id fits uint32 (like Wikipedia ids)\n",
        "        f_idx.write(REC.pack(doc_id, offset, length))\n",
        "        offset += length\n",
        "\n",
        "print(\"Local TSV/IDX:\", LOCAL_TSV, LOCAL_IDX)\n",
        "\n",
        "# Upload final files\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_TSV, PR_TSV_GCS])\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_IDX, PR_IDX_GCS])\n",
        "\n",
        "print(\"Uploaded:\", PR_TSV_GCS, PR_IDX_GCS)\n"
      ],
      "metadata": {
        "id": "n0gZl7aE8_Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pageviews.pkl**"
      ],
      "metadata": {
        "id": "b_zti5R6PD-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# just as described in assignment 1 + write to TSV and than create idx\n",
        "pv_path = \"https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2\"\n",
        "p = Path(pv_path)\n",
        "pv_name = p.name\n",
        "pv_temp = f\"{p.stem}-4dedup.txt\"\n",
        "\n",
        "# download\n",
        "!wget -N $pv_path\n",
        "\n",
        "# filter english + keep article_id + views (fields 3 and 5) + remove lines with article id or page view values that are not a sequence of digits.\n",
        "!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n",
        "\n",
        "# counter creation and save\n",
        "\n",
        "#wid2pv = Counter()\n",
        "#with open(pv_temp, \"rt\") as f:\n",
        " #   for line in f:\n",
        "  #      parts = line.split()\n",
        "   #     wid2pv.update({int(parts[0]): int(parts[1])})\n",
        "\n",
        "BASE_GCS_DIR = \"ir_project_indexes\"\n",
        "PV_TMP_DIR   = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/pageviews_tsv_parts\"\n",
        "PV_TSV_GCS   = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/pageviews.tsv\"\n",
        "PV_IDX_GCS   = f\"gs://{BUCKET_NAME}/{BASE_GCS_DIR}/pageviews.idx\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AEcrmIhFPH9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "lines = spark.read.text(pv_temp)\n",
        "\n",
        "pv_df = (lines.select(F.split(F.col(\"value\"), r\"\\s+\").alias(\"parts\")).filter(F.size(\"parts\") == 2)\n",
        "    .select(F.col(\"parts\").getItem(0).cast(\"long\").alias(\"id\"),F.col(\"parts\").getItem(1).cast(\"long\").alias(\"views\")))\n",
        "\n",
        "pv_agg = pv_df.groupBy(\"id\").agg(F.sum(\"views\").alias(\"views\")).sort(\"id\")\n",
        "\n",
        "pv_agg.selectExpr(\"concat(cast(id as string), '\\t', cast(views as string)) as line\").write.mode(\"overwrite\").text(PV_TMP_DIR)\n",
        "\n",
        "print(\"Wrote PageViews TSV parts to:\", PV_TMP_DIR)\n",
        "print(\"Will upload final TSV/IDX to:\", PV_TSV_GCS, PV_IDX_GCS)\n"
      ],
      "metadata": {
        "id": "q1HQFT9ocMBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, struct, subprocess, tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "LOCAL_DIR = tempfile.mkdtemp(prefix=\"pageviews_build_\")\n",
        "LOCAL_TSV = os.path.join(LOCAL_DIR, \"pageviews.tsv\")\n",
        "LOCAL_IDX = os.path.join(LOCAL_DIR, \"pageviews.idx\")\n",
        "\n",
        "subprocess.check_call([\"gsutil\", \"-m\", \"cp\", f\"{PV_TMP_DIR}/part-*\", LOCAL_DIR])\n",
        "parts = sorted(glob.glob(os.path.join(LOCAL_DIR, \"part-*\")))\n",
        "\n",
        "with open(LOCAL_TSV, \"wb\") as out:\n",
        "    for p in parts:\n",
        "        with open(p, \"rb\") as f:\n",
        "            for line in f:\n",
        "                out.write(line.rstrip(b\"\\n\") + b\"\\n\")\n",
        "\n",
        "# IDX record: uint32 doc_id | uint64 offset | uint32 length\n",
        "REC = struct.Struct(\"<I Q I\")\n",
        "\n",
        "with open(LOCAL_TSV, \"rb\") as f_tsv, open(LOCAL_IDX, \"wb\") as f_idx:\n",
        "    offset = 0\n",
        "    for line in f_tsv:\n",
        "        length = len(line)\n",
        "        tab = line.find(b\"\\t\")\n",
        "        if tab < 0:\n",
        "            offset += length\n",
        "            continue\n",
        "        doc_id = int(line[:tab])\n",
        "        f_idx.write(REC.pack(doc_id, offset, length))\n",
        "        offset += length\n",
        "\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_TSV, PV_TSV_GCS])\n",
        "subprocess.check_call([\"gsutil\", \"cp\", LOCAL_IDX, PV_IDX_GCS])\n",
        "print(\"Uploaded:\", PV_TSV_GCS, PV_IDX_GCS)\n"
      ],
      "metadata": {
        "id": "nibKmNrkcWma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check:\n",
        "prev = -1\n",
        "bad_at = None\n",
        "line_no = 0\n",
        "\n",
        "with open(LOCAL_TSV, \"rb\") as f:\n",
        "    for line in f:\n",
        "        line_no += 1\n",
        "        tab = line.find(b\"\\t\")\n",
        "        if tab < 0:\n",
        "            continue\n",
        "        doc_id = int(line[:tab])\n",
        "        if doc_id < prev:\n",
        "            bad_at = (prev, doc_id, line_no)\n",
        "            break\n",
        "        prev = doc_id\n",
        "\n",
        "print(\"OK sorted\" if bad_at is None else f\"NOT sorted at line {bad_at[2]}: {bad_at[0]} -> {bad_at[1]}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "PNVmo7i1gz4f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}